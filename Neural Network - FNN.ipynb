{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network - FNN\n",
    "\n",
    "BRZOZOWSKI MAREK\n",
    "\n",
    "A cars dataset will be used to explore:\n",
    "\n",
    "Predicting Car sales using Neural Networks \n",
    "\n",
    "\n",
    "Neural Networks are a series algorithms for building a computer program that learns from data. It loosely resembles the way our human brains operate. Neurons in the simplest form are links that activate on certain responses whether chemical signals or data inputs for computers. As the brain evolves to create new linking neurons so to does nequral networks as they adapt to changing inputs. \n",
    "\n",
    "Feedforward neural networks are artifical neural networks that do not form a cycle. It was one of the earliest and simplest neural networks developed. The direction moves only one way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Loading Packages and Libraries\n",
    "\n",
    "import numpy as np \n",
    "import sklearn\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data Set\n",
    "dataset = np.loadtxt('cars.csv',delimiter= ',')\n",
    "\n",
    "# Train Test Split\n",
    "x = dataset[:, 0:5]\n",
    "y = dataset[ :, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\nMinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "# Shaping and Scaling\n",
    "\n",
    "# Reshaping testing data into a single list column\n",
    "y = np.reshape( y, (-1,1))\n",
    "\n",
    "# Scaling using MinMaxScaler = scaking each feature to a given range. \n",
    "# Default is between zero and one.\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(x))\n",
    "print(scaler.fit(y))\n",
    "\n",
    "xscale = scaler.transform(x)\n",
    "yscale = scaler.transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting scaled data into test and training \n",
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "source": [
    "# Notes\n",
    "We have 5 input neurons, 1 output neurons, a training data set of 722 and we use 10 as our arbitrary scaling factor. Thus we have a shape of 12. Next layer has 2/3 of the first layer, 8.\n",
    "\n",
    "Sequential = Models are linear stack of layers, treat each layer as object that feed into the next. \n",
    "\n",
    "dense = densly connected NN layer. There are other layers such as LSTM, Poooling, Convolutional, etc..\n",
    "\n",
    "kernel_intializer = Intializer for the kernal weights matrix. A normal distribution is used to initialize the weights.\n",
    "\n",
    "activation = Indications whether or not a signal should be processed. Relu is the standard to use in hidden layers. \n",
    "\n",
    "Since we are working with continous values, the linear activation is sufficient for the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 12)                72        \n_________________________________________________________________\ndense_2 (Dense)              (None, 8)                 104       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 9         \n=================================================================\nTotal params: 185\nTrainable params: 185\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 5,\n",
    "                kernel_initializer='normal',\n",
    "                activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "source": [
    "# Notes\n",
    "\n",
    "loss = Quantifies the agreement between the predicted output and the ground output. Determines the penalty for an incorrect classification. Our Problem is a Regression Type, with Numerical values, and we are using a Linear activation function; therefore, we will use Mean Squared Error and Mean Absolute Error.\n",
    "\n",
    "optimizer = Optimizing the Gradient Descent. Adam is staple.  \n",
    "Adam optimization algorithm is an extension to stochastic gradient descent.\n",
    "\n",
    "metric = Used to judge the performance of the model. As we said above we will be using MSE and MAE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model with the measurements metrics and optimizers.\n",
    "model.compile(\n",
    "             loss = 'mean_squared_error',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['MSE','MAE']\n",
    "             )\n"
   ]
  },
  {
   "source": [
    "# Notes\n",
    "Epochs = Cycle of presentation of the entire training set. \n",
    "Too few epochs cause underfitting. Too many epochs cause overfitting.\n",
    "\n",
    "batch_size = Defines the number of samples that will propagate through the network. Less memory, works faster with batches. \n",
    "The smaller the batch the less accurate the estimate of the gradient will be.\n",
    "\n",
    "validation_split = Uses a fraction of the training data to validate the model. \n",
    "verbose = Progress bar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_error: 0.1345\n",
      "Epoch 64/150\n",
      "577/577 [==============================] - 0s 63us/step - loss: 0.0197 - mean_squared_error: 0.0197 - mean_absolute_error: 0.1103 - val_loss: 0.0264 - val_mean_squared_error: 0.0264 - val_mean_absolute_error: 0.1342\n",
      "Epoch 65/150\n",
      "577/577 [==============================] - 0s 65us/step - loss: 0.0196 - mean_squared_error: 0.0196 - mean_absolute_error: 0.1099 - val_loss: 0.0263 - val_mean_squared_error: 0.0263 - val_mean_absolute_error: 0.1338\n",
      "Epoch 66/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0196 - mean_squared_error: 0.0196 - mean_absolute_error: 0.1094 - val_loss: 0.0264 - val_mean_squared_error: 0.0264 - val_mean_absolute_error: 0.1333\n",
      "Epoch 67/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0195 - mean_squared_error: 0.0195 - mean_absolute_error: 0.1092 - val_loss: 0.0261 - val_mean_squared_error: 0.0261 - val_mean_absolute_error: 0.1334\n",
      "Epoch 68/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0195 - mean_squared_error: 0.0195 - mean_absolute_error: 0.1095 - val_loss: 0.0261 - val_mean_squared_error: 0.0261 - val_mean_absolute_error: 0.1333\n",
      "Epoch 69/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0194 - mean_squared_error: 0.0194 - mean_absolute_error: 0.1088 - val_loss: 0.0261 - val_mean_squared_error: 0.0261 - val_mean_absolute_error: 0.1328\n",
      "Epoch 70/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0194 - mean_squared_error: 0.0194 - mean_absolute_error: 0.1085 - val_loss: 0.0260 - val_mean_squared_error: 0.0260 - val_mean_absolute_error: 0.1325\n",
      "Epoch 71/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0193 - mean_squared_error: 0.0193 - mean_absolute_error: 0.1083 - val_loss: 0.0259 - val_mean_squared_error: 0.0259 - val_mean_absolute_error: 0.1322\n",
      "Epoch 72/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0193 - mean_squared_error: 0.0193 - mean_absolute_error: 0.1083 - val_loss: 0.0258 - val_mean_squared_error: 0.0258 - val_mean_absolute_error: 0.1322\n",
      "Epoch 73/150\n",
      "577/577 [==============================] - 0s 57us/step - loss: 0.0192 - mean_squared_error: 0.0192 - mean_absolute_error: 0.1078 - val_loss: 0.0258 - val_mean_squared_error: 0.0258 - val_mean_absolute_error: 0.1317\n",
      "Epoch 74/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0192 - mean_squared_error: 0.0192 - mean_absolute_error: 0.1076 - val_loss: 0.0257 - val_mean_squared_error: 0.0257 - val_mean_absolute_error: 0.1315\n",
      "Epoch 75/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0192 - mean_squared_error: 0.0192 - mean_absolute_error: 0.1072 - val_loss: 0.0257 - val_mean_squared_error: 0.0257 - val_mean_absolute_error: 0.1312\n",
      "Epoch 76/150\n",
      "577/577 [==============================] - 0s 56us/step - loss: 0.0191 - mean_squared_error: 0.0191 - mean_absolute_error: 0.1069 - val_loss: 0.0256 - val_mean_squared_error: 0.0256 - val_mean_absolute_error: 0.1310\n",
      "Epoch 77/150\n",
      "577/577 [==============================] - 0s 62us/step - loss: 0.0191 - mean_squared_error: 0.0191 - mean_absolute_error: 0.1071 - val_loss: 0.0255 - val_mean_squared_error: 0.0255 - val_mean_absolute_error: 0.1310\n",
      "Epoch 78/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0190 - mean_squared_error: 0.0190 - mean_absolute_error: 0.1069 - val_loss: 0.0254 - val_mean_squared_error: 0.0254 - val_mean_absolute_error: 0.1307\n",
      "Epoch 79/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0190 - mean_squared_error: 0.0190 - mean_absolute_error: 0.1064 - val_loss: 0.0254 - val_mean_squared_error: 0.0254 - val_mean_absolute_error: 0.1305\n",
      "Epoch 80/150\n",
      "577/577 [==============================] - 0s 65us/step - loss: 0.0190 - mean_squared_error: 0.0190 - mean_absolute_error: 0.1062 - val_loss: 0.0253 - val_mean_squared_error: 0.0253 - val_mean_absolute_error: 0.1304\n",
      "Epoch 81/150\n",
      "577/577 [==============================] - 0s 61us/step - loss: 0.0189 - mean_squared_error: 0.0189 - mean_absolute_error: 0.1064 - val_loss: 0.0252 - val_mean_squared_error: 0.0252 - val_mean_absolute_error: 0.1302\n",
      "Epoch 82/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0189 - mean_squared_error: 0.0189 - mean_absolute_error: 0.1059 - val_loss: 0.0252 - val_mean_squared_error: 0.0252 - val_mean_absolute_error: 0.1298\n",
      "Epoch 83/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0188 - mean_squared_error: 0.0188 - mean_absolute_error: 0.1055 - val_loss: 0.0252 - val_mean_squared_error: 0.0252 - val_mean_absolute_error: 0.1296\n",
      "Epoch 84/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0188 - mean_squared_error: 0.0188 - mean_absolute_error: 0.1053 - val_loss: 0.0251 - val_mean_squared_error: 0.0251 - val_mean_absolute_error: 0.1294\n",
      "Epoch 85/150\n",
      "577/577 [==============================] - 0s 60us/step - loss: 0.0188 - mean_squared_error: 0.0188 - mean_absolute_error: 0.1055 - val_loss: 0.0250 - val_mean_squared_error: 0.0250 - val_mean_absolute_error: 0.1294\n",
      "Epoch 86/150\n",
      "577/577 [==============================] - 0s 59us/step - loss: 0.0188 - mean_squared_error: 0.0188 - mean_absolute_error: 0.1051 - val_loss: 0.0250 - val_mean_squared_error: 0.0250 - val_mean_absolute_error: 0.1290\n",
      "Epoch 87/150\n",
      "577/577 [==============================] - 0s 66us/step - loss: 0.0188 - mean_squared_error: 0.0188 - mean_absolute_error: 0.1046 - val_loss: 0.0250 - val_mean_squared_error: 0.0250 - val_mean_absolute_error: 0.1288\n",
      "Epoch 88/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0187 - mean_squared_error: 0.0187 - mean_absolute_error: 0.1044 - val_loss: 0.0248 - val_mean_squared_error: 0.0248 - val_mean_absolute_error: 0.1288\n",
      "Epoch 89/150\n",
      "577/577 [==============================] - 0s 71us/step - loss: 0.0187 - mean_squared_error: 0.0187 - mean_absolute_error: 0.1048 - val_loss: 0.0248 - val_mean_squared_error: 0.0248 - val_mean_absolute_error: 0.1286\n",
      "Epoch 90/150\n",
      "577/577 [==============================] - 0s 76us/step - loss: 0.0187 - mean_squared_error: 0.0187 - mean_absolute_error: 0.1041 - val_loss: 0.0249 - val_mean_squared_error: 0.0249 - val_mean_absolute_error: 0.1282\n",
      "Epoch 91/150\n",
      "577/577 [==============================] - 0s 93us/step - loss: 0.0186 - mean_squared_error: 0.0186 - mean_absolute_error: 0.1040 - val_loss: 0.0247 - val_mean_squared_error: 0.0247 - val_mean_absolute_error: 0.1282\n",
      "Epoch 92/150\n",
      "577/577 [==============================] - 0s 124us/step - loss: 0.0186 - mean_squared_error: 0.0186 - mean_absolute_error: 0.1046 - val_loss: 0.0246 - val_mean_squared_error: 0.0246 - val_mean_absolute_error: 0.1282\n",
      "Epoch 93/150\n",
      "577/577 [==============================] - 0s 124us/step - loss: 0.0186 - mean_squared_error: 0.0186 - mean_absolute_error: 0.1037 - val_loss: 0.0248 - val_mean_squared_error: 0.0248 - val_mean_absolute_error: 0.1278\n",
      "Epoch 94/150\n",
      "577/577 [==============================] - 0s 129us/step - loss: 0.0186 - mean_squared_error: 0.0186 - mean_absolute_error: 0.1034 - val_loss: 0.0246 - val_mean_squared_error: 0.0246 - val_mean_absolute_error: 0.1277\n",
      "Epoch 95/150\n",
      "577/577 [==============================] - 0s 175us/step - loss: 0.0185 - mean_squared_error: 0.0185 - mean_absolute_error: 0.1037 - val_loss: 0.0245 - val_mean_squared_error: 0.0245 - val_mean_absolute_error: 0.1276\n",
      "Epoch 96/150\n",
      "577/577 [==============================] - 0s 146us/step - loss: 0.0185 - mean_squared_error: 0.0185 - mean_absolute_error: 0.1031 - val_loss: 0.0246 - val_mean_squared_error: 0.0246 - val_mean_absolute_error: 0.1273\n",
      "Epoch 97/150\n",
      "577/577 [==============================] - 0s 134us/step - loss: 0.0185 - mean_squared_error: 0.0185 - mean_absolute_error: 0.1030 - val_loss: 0.0245 - val_mean_squared_error: 0.0245 - val_mean_absolute_error: 0.1272\n",
      "Epoch 98/150\n",
      "577/577 [==============================] - 0s 139us/step - loss: 0.0185 - mean_squared_error: 0.0185 - mean_absolute_error: 0.1028 - val_loss: 0.0244 - val_mean_squared_error: 0.0244 - val_mean_absolute_error: 0.1270\n",
      "Epoch 99/150\n",
      "577/577 [==============================] - 0s 120us/step - loss: 0.0184 - mean_squared_error: 0.0184 - mean_absolute_error: 0.1027 - val_loss: 0.0244 - val_mean_squared_error: 0.0244 - val_mean_absolute_error: 0.1268\n",
      "Epoch 100/150\n",
      "577/577 [==============================] - 0s 135us/step - loss: 0.0185 - mean_squared_error: 0.0185 - mean_absolute_error: 0.1023 - val_loss: 0.0244 - val_mean_squared_error: 0.0244 - val_mean_absolute_error: 0.1266\n",
      "Epoch 101/150\n",
      "577/577 [==============================] - 0s 85us/step - loss: 0.0184 - mean_squared_error: 0.0184 - mean_absolute_error: 0.1026 - val_loss: 0.0243 - val_mean_squared_error: 0.0243 - val_mean_absolute_error: 0.1266\n",
      "Epoch 102/150\n",
      "577/577 [==============================] - 0s 81us/step - loss: 0.0184 - mean_squared_error: 0.0184 - mean_absolute_error: 0.1023 - val_loss: 0.0243 - val_mean_squared_error: 0.0243 - val_mean_absolute_error: 0.1264\n",
      "Epoch 103/150\n",
      "577/577 [==============================] - 0s 85us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1020 - val_loss: 0.0243 - val_mean_squared_error: 0.0243 - val_mean_absolute_error: 0.1263\n",
      "Epoch 104/150\n",
      "577/577 [==============================] - 0s 85us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1018 - val_loss: 0.0243 - val_mean_squared_error: 0.0243 - val_mean_absolute_error: 0.1261\n",
      "Epoch 105/150\n",
      "577/577 [==============================] - 0s 77us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1017 - val_loss: 0.0241 - val_mean_squared_error: 0.0241 - val_mean_absolute_error: 0.1261\n",
      "Epoch 106/150\n",
      "577/577 [==============================] - 0s 81us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1020 - val_loss: 0.0241 - val_mean_squared_error: 0.0241 - val_mean_absolute_error: 0.1260\n",
      "Epoch 107/150\n",
      "577/577 [==============================] - 0s 76us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1017 - val_loss: 0.0241 - val_mean_squared_error: 0.0241 - val_mean_absolute_error: 0.1257\n",
      "Epoch 108/150\n",
      "577/577 [==============================] - 0s 73us/step - loss: 0.0183 - mean_squared_error: 0.0183 - mean_absolute_error: 0.1014 - val_loss: 0.0241 - val_mean_squared_error: 0.0241 - val_mean_absolute_error: 0.1256\n",
      "Epoch 109/150\n",
      "577/577 [==============================] - 0s 74us/step - loss: 0.0182 - mean_squared_error: 0.0182 - mean_absolute_error: 0.1013 - val_loss: 0.0240 - val_mean_squared_error: 0.0240 - val_mean_absolute_error: 0.1255\n",
      "Epoch 110/150\n",
      "577/577 [==============================] - 0s 74us/step - loss: 0.0182 - mean_squared_error: 0.0182 - mean_absolute_error: 0.1013 - val_loss: 0.0240 - val_mean_squared_error: 0.0240 - val_mean_absolute_error: 0.1253\n",
      "Epoch 111/150\n",
      "577/577 [==============================] - 0s 73us/step - loss: 0.0182 - mean_squared_error: 0.0182 - mean_absolute_error: 0.1011 - val_loss: 0.0240 - val_mean_squared_error: 0.0240 - val_mean_absolute_error: 0.1251\n",
      "Epoch 112/150\n",
      "577/577 [==============================] - 0s 74us/step - loss: 0.0182 - mean_squared_error: 0.0182 - mean_absolute_error: 0.1009 - val_loss: 0.0239 - val_mean_squared_error: 0.0239 - val_mean_absolute_error: 0.1250\n",
      "Epoch 113/150\n",
      "577/577 [==============================] - 0s 88us/step - loss: 0.0182 - mean_squared_error: 0.0182 - mean_absolute_error: 0.1006 - val_loss: 0.0239 - val_mean_squared_error: 0.0239 - val_mean_absolute_error: 0.1248\n",
      "Epoch 114/150\n",
      "577/577 [==============================] - 0s 79us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1005 - val_loss: 0.0239 - val_mean_squared_error: 0.0239 - val_mean_absolute_error: 0.1247\n",
      "Epoch 115/150\n",
      "577/577 [==============================] - 0s 119us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1004 - val_loss: 0.0238 - val_mean_squared_error: 0.0238 - val_mean_absolute_error: 0.1247\n",
      "Epoch 116/150\n",
      "577/577 [==============================] - 0s 84us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1004 - val_loss: 0.0238 - val_mean_squared_error: 0.0238 - val_mean_absolute_error: 0.1246\n",
      "Epoch 117/150\n",
      "577/577 [==============================] - 0s 82us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1001 - val_loss: 0.0238 - val_mean_squared_error: 0.0238 - val_mean_absolute_error: 0.1244\n",
      "Epoch 118/150\n",
      "577/577 [==============================] - 0s 80us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1000 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1244\n",
      "Epoch 119/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.1001 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1241\n",
      "Epoch 120/150\n",
      "577/577 [==============================] - 0s 63us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.0997 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1240\n",
      "Epoch 121/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.0997 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1239\n",
      "Epoch 122/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0995 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1239\n",
      "Epoch 123/150\n",
      "577/577 [==============================] - 0s 76us/step - loss: 0.0181 - mean_squared_error: 0.0181 - mean_absolute_error: 0.0994 - val_loss: 0.0237 - val_mean_squared_error: 0.0237 - val_mean_absolute_error: 0.1237\n",
      "Epoch 124/150\n",
      "577/577 [==============================] - 0s 80us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0995 - val_loss: 0.0236 - val_mean_squared_error: 0.0236 - val_mean_absolute_error: 0.1238\n",
      "Epoch 125/150\n",
      "577/577 [==============================] - 0s 78us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0992 - val_loss: 0.0236 - val_mean_squared_error: 0.0236 - val_mean_absolute_error: 0.1235\n",
      "Epoch 126/150\n",
      "577/577 [==============================] - 0s 72us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0991 - val_loss: 0.0236 - val_mean_squared_error: 0.0236 - val_mean_absolute_error: 0.1234\n",
      "Epoch 127/150\n",
      "577/577 [==============================] - 0s 123us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0994 - val_loss: 0.0235 - val_mean_squared_error: 0.0235 - val_mean_absolute_error: 0.1235\n",
      "Epoch 128/150\n",
      "577/577 [==============================] - 0s 73us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0990 - val_loss: 0.0236 - val_mean_squared_error: 0.0236 - val_mean_absolute_error: 0.1232\n",
      "Epoch 129/150\n",
      "577/577 [==============================] - 0s 70us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0990 - val_loss: 0.0235 - val_mean_squared_error: 0.0235 - val_mean_absolute_error: 0.1232\n",
      "Epoch 130/150\n",
      "577/577 [==============================] - 0s 64us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0989 - val_loss: 0.0234 - val_mean_squared_error: 0.0234 - val_mean_absolute_error: 0.1231\n",
      "Epoch 131/150\n",
      "577/577 [==============================] - 0s 74us/step - loss: 0.0180 - mean_squared_error: 0.0180 - mean_absolute_error: 0.0995 - val_loss: 0.0234 - val_mean_squared_error: 0.0234 - val_mean_absolute_error: 0.1231\n",
      "Epoch 132/150\n",
      "577/577 [==============================] - 0s 81us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0988 - val_loss: 0.0235 - val_mean_squared_error: 0.0235 - val_mean_absolute_error: 0.1228\n",
      "Epoch 133/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0984 - val_loss: 0.0234 - val_mean_squared_error: 0.0234 - val_mean_absolute_error: 0.1227\n",
      "Epoch 134/150\n",
      "577/577 [==============================] - 0s 65us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0986 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1228\n",
      "Epoch 135/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0985 - val_loss: 0.0234 - val_mean_squared_error: 0.0234 - val_mean_absolute_error: 0.1225\n",
      "Epoch 136/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0179 - mean_squared_error: 0.0179 - mean_absolute_error: 0.0982 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1225\n",
      "Epoch 137/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0982 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1223\n",
      "Epoch 138/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0981 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1223\n",
      "Epoch 139/150\n",
      "577/577 [==============================] - 0s 68us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0980 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1222\n",
      "Epoch 140/150\n",
      "577/577 [==============================] - 0s 73us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0979 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1220\n",
      "Epoch 141/150\n",
      "577/577 [==============================] - 0s 79us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0979 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1221\n",
      "Epoch 142/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0981 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1219\n",
      "Epoch 143/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0977 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1218\n",
      "Epoch 144/150\n",
      "577/577 [==============================] - 0s 72us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0976 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1218\n",
      "Epoch 145/150\n",
      "577/577 [==============================] - 0s 67us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0978 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1218\n",
      "Epoch 146/150\n",
      "577/577 [==============================] - 0s 70us/step - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.0974 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1216\n",
      "Epoch 147/150\n",
      "577/577 [==============================] - 0s 66us/step - loss: 0.0177 - mean_squared_error: 0.0177 - mean_absolute_error: 0.0973 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1215\n",
      "Epoch 148/150\n",
      "577/577 [==============================] - 0s 69us/step - loss: 0.0177 - mean_squared_error: 0.0177 - mean_absolute_error: 0.0975 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1215\n",
      "Epoch 149/150\n",
      "577/577 [==============================] - 0s 76us/step - loss: 0.0177 - mean_squared_error: 0.0177 - mean_absolute_error: 0.0972 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1213\n",
      "Epoch 150/150\n",
      "577/577 [==============================] - 0s 66us/step - loss: 0.0177 - mean_squared_error: 0.0177 - mean_absolute_error: 0.0971 - val_loss: 0.0230 - val_mean_squared_error: 0.0230 - val_mean_absolute_error: 0.1213\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19db014a128>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Training the model using the .fit function\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 150,\n",
    "    batch_size = 50, \n",
    "    validation_split = 0.2,\n",
    "    verbose= 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "241/241 [==============================] - 0s 54us/step\n",
      "Loss -> 0.01725184926724533 MSE -> 0.01725184926724533 MAE --> 0.10155448336076935\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Evaluating our model\n",
    "# Evaluating how well our model works. The smaller value \n",
    "# the better.\n",
    "# Return the loss value & metrics\n",
    "loss , mse, mae = ((model.evaluate(X_test,y_test)))\n",
    "print('Loss -> %s MSE -> %s MAE --> %s'  %(loss,mse,mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.01725184973844122"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Method 2: Another method of evaluating our model\n",
    "# Predicting the X_test results.\n",
    "predictions = model.predict(X_test)\n",
    "# Evaluating how well our model works. The smaller value the better.\n",
    "sklearn.metrics.mean_squared_error(predictions,y_test)\n",
    "\n",
    "# Results are the same as Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For the new set of data -> [  40.    0.   26. 9000. 8000.] Predicted -> [12030.366]\n"
     ]
    }
   ],
   "source": [
    "# Predict the new set of data\n",
    "Xnew = np.array([[40,0,26,9000,8000]])\n",
    "\n",
    "# Scale the new vector\n",
    "Xnew = scaler.transform(Xnew)\n",
    "\n",
    "# Predict the new value using the model\n",
    "ynew = model.predict(Xnew)\n",
    "\n",
    "# Transform back to the original data format. \n",
    "# Use .inverse_transform on the scale function\n",
    "Xnew = scaler.inverse_transform(Xnew)\n",
    "ynew = scaler.inverse_transform(ynew)\n",
    "\n",
    "print('For the new set of data -> %s Predicted -> %s'  % (Xnew[0], ynew[0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python36864bitbasecondab7b0b3c2e5114e4588b3c6978978b1d3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}